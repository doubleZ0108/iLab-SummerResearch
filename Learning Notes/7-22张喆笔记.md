# ilab医学影像暑期科研

2019-07-22 张喆学习笔记

[TOC]

# 第一门课 神经网路和深度学习

## 第二周: 神经网络的编程基础

### 导数

> 在f(a) = 3a例子中, 无论当a=2还是a=5时, a向右偏移0.001使得f(a)增加的量比上a增加的量都是3
>
> 更直观的来看是该直线的斜率是3, 也就是小三角形的高/宽是3
>
> ![image.png](https://upload-images.jianshu.io/upload_images/12014150-89f3e85acb84fb1b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

f(a)增大的值为点在a处的斜率或导数, 乘以向右移动的距离

*(这里需要注意, 导数增大的值, 不是等于的导数公式算出来的值, 二十根据导数算出来的一个估计值)*

例f(a) = a<sup>3</sup>    f<sup>'</sup>(a) = 3a<sup>2</sup>, 令a=2, 则a<sup>3</sup>=8, 如果a增大0.001, f<sup>'</sup>(2)=12, 所以f(a)变大12*0.001 = 0.012, f(a)=8.012 这和真实的2.001<sup>3</sup>十分接近

### 计算图

一个神经网络的计算, 都是按照前向或反向传播过程组织的

- 首先计算出一个新的网络的输出(前向过程)
- 紧接着进行一个反向传输操作 --> 用来计算出对应的梯度或导数

#### 计算图的代价函数计算

例如, 函数为3(a + bc), 计算步骤为

1. 计算b*c, 把它储存在u中
2. 计算v=a+u
3. 最后输出J=3v

![image.png](https://upload-images.jianshu.io/upload_images/12014150-721809d96e2ce459.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

*(蓝线从左到右是计算代价函数J的步骤, 红线从右到左是计算导数的方式)*

#### 计算图的导数计算

$\frac{d J}{d u}=\frac{d J}{d v} \frac{d v}{d u}$

$\frac{d J}{d b}=\frac{d J}{d u} \frac{d u}{d b}$

$\frac{d J}{d v} \frac{d v}{d a}$

**详细解释下$\frac{d J}{d a} $的计算:** 

例子中a=5, 我们让它编程概念5.001, 那么对v的影响就是a+u, 之前v=11, 现在变成11.001, J因此变成33.003

当a增加0.001, J增加0.003, 因此$\frac{d J}{d a} $=3

(如果改变a, 那么也会改变v, 如果改变v, 也会改变J, 所以J的净变化量就是当把a提高0.001时变化的0.003)

#### 符号约定

- **dvar:** 表示输出变量对变量var的导数, 是$\frac{d FinalOutputvar}{d var} $ --> dFinalOutputvar_dvar的简写

  例如上面的$\frac{d J}{d a} $ 用变量**da**表示

### 逻辑回归中的梯度下降

$\hat{y}=a=\sigma(z)$, 其中$z=w^{T} x+b$, $\sigma(z)=\frac{1}{1+e^{-z}}$

损失函数: $L\left(\hat{y}^{(i)}, y^{(i)}\right)=-y^{(i)} \log \hat{y}^{(i)}-\left(1-y^{(i)}\right) \log \left(1-\hat{y}^{(i)}\right)$

代价函数: $J(w, b)=\frac{1}{m} \sum_{i}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)$

梯度下降法:

- w的修正值: $W :=W-a \frac{\partial J(w, b)}{\partial w}$
- b的修正值: $b :=b-a \frac{\partial J(w, b)}{\partial b}$

#### 只考虑单个样本的情况:

假设只有两个特征x1和x2, 为了计算z, 我们需要输入参数w1, w2和b

$Z=w_{1} x_{1}+w_{2} x_{2}+b$

单个样本的代价函数为$L(a, y)=-(y \log (a)+(1-y) \log (1-a))$, a是逻辑回归的输出, y是样本的标签值

![image.png](https://upload-images.jianshu.io/upload_images/12014150-89b6269f53ba91dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**结果:**

- da: $\frac{d L(a, y)}{d a}=-y / a+(1-y) /(1-a)$
- dz:$\frac{d L(a, y)}{d z}=\frac{d L}{d z}=\left(\frac{d L}{d a}\right) \cdot\left(\frac{d a}{d z}\right)=\left(-\frac{y}{a}+\frac{(1-y)}{(1-a)}\right) \cdot a(1-a)=a-y$
- dw1:$\frac{1}{m} \sum_{i}^{m} x_{1}^{(i)}\left(a^{(i)}-y^{(i)}\right)$
- dw2: $\frac{1}{m} \sum_{i}^{m} x_{2}^{(i)}\left(a^{(i)}-y^{(i)}\right)$
- db: $\frac{1}{m} \sum_{i}^{m}\left(a^{(i)}-y^{(i)}\right)$

#### $\infty$要记住的公式

- 计算dz: dz = (a - y)
- 计算dw1: dw1 = x1 * dz
- 计算dw2: dw2 = x2 * dz
- 计算db: db = dz
- 更新w1: w1 = w1 - a * dw1
- 更新w2: w2 = w2 - a * dw2
- 更新b: b = b - a * db

#### m个样本的梯度下降: 

```python
J,dw1,dw2,db = 0,0,0,0
for i in range(0,m):
    z[i] = w*x[i] + b
    a[i] = sigmoid(z[i])
    J += -(y[i]*log(a[i]) + (1-y[i])*log(1-a[i]))
    dz[i] = a[i] - y[i]
    dw1 += x1[i] * dz[i]
    dw2 += x2[i] * dz[i]
    db += dz[i]
J /= m
dw1 /= m
dw2 /= m
db /=m

w -= alpha * dw
b -= alpha * db
```

- 算法中不要显示的使用for循环 --> 向量技术 (为了适应越来越大的数据集)