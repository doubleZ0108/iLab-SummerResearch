# ilab医学影像暑期科研

2019-07-20 张喆学习笔记

[TOC]

# 第一门课 神经网路和深度学习

## 第二周: 神经网络的编程基础

1. 如何处理训练集 --> 打破习惯的for循环遍历训练集的每个样本
2. 神经网络训练过程: 前向暂停, 前向传播 --> 反向暂停, 反向传播
3. 逻辑回归

### 二分类

> 引例: 一张彩色图片是不是猫?
>
> 输入: 一张图片, 也就是三个64*64的矩阵
>
> 输出: 结果标签(0或1), 用于表示是否为猫
>
> 我们再定义一个n<sub>x</sub>表示输入特征向量的维度, 这里我们采用的规则是把rgb三个矩阵中的每一个数值都一次排列, 最终形成的是一个64 * 64 * 3的一维向量, 构成我们的额特征向量x; 我们构建的神经网络就是通过这个12288维的向量作为输入, 输出0或1, 也就是预测图片中是否有猫

#### 符号定义

- **x:** 输入数据, 维度是 <u>n<sub>x</sub> * 1</u>

- **y:** 输出结果, 取值是<u>(0, 1)</u>

- **X = [x<sup>(1)</sup>, x<sup>(2)</sup>, ... , x<sup>(m)</sup>]:** 所有训练数据集的输入值, 维度是 <u>n<sub>x</sub> * m</u>

  *(如果X中的项是按行给出的, 我们可以对其转置, 神经网络中一般采用👇这种形式)*

  ![image.png](https://upload-images.jianshu.io/upload_images/12014150-0041d596e602d544.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- **Y = [y<sup>(1)</sup>, y<sup>(2)</sup>, ... , y<sup>(m)</sup>]:** 所有训练数据集的输出值, 维度是 <u>1 * m</u>

  ![image.png](https://upload-images.jianshu.io/upload_images/12014150-cdad0bb9f8dc5ebb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- **(x<sup>(i)</sup>, y<sup>(i)</sup>):** 第i个单独的样本(输入/输出)

- **M<sub>train</sub>:** 训练样本的个数

- **M<sub>test</sub>:** 测试集的样本数

ps. 可以通过`X.shape`获取**X**矩阵的规模(n<sub>x</sub>, m), `Y.shape`获取**Y**矩阵的规模(1, m)

### 逻辑回归 

#### 假设函数

- $\hat{y}$表示y等于1的一种可能性, 也就是对实际值y的估计
- w表示逻辑回归的参数, 特征权重, 维度为n<sub>x</sub>, 与特征向量相同
- b表示偏差, 是一个实数

如果令 $\hat{y}$ = w<sup>T</sup>x + b 作为y的预测值, 这个值不在[0,1]区间内, 是没有意义的, 因此我们的输出应该将这个式子作为自变量的`sigmoid`函数, 将线性函数转换为非线性函数

![image.png](https://upload-images.jianshu.io/upload_images/12014150-e4808ee939db46f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

#### sigmoid函数

![image.png](https://upload-images.jianshu.io/upload_images/12014150-9e5d09e566b05a69.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 值域平滑的从0走到1
- 纵轴的截距是0.5

**目标是让机器学习参数w和b使的$\hat{y}$成为y=1这一情况概率的一个很好的估计**

### 代价函数

通过训练代价函数可以得到参数w和参数b => 进而训练逻辑回归模型

#### 损失函数

 L($\hat{y}$, y), 也叫误差函数, 用来衡量预测输出值和实际值有多接近 --> 算法的运行情况

>  一般使用与测试和实际值的平方差或者平方差的一半, 但是由于我们的优化目标不是凸优化, 只能找到多个局部最优值

L($\hat{y}$, y) = -ylog($\hat{y}$) - (1 - y)log(1 - $\hat{y}$)

我们希望逻辑回归损失函数尽可能小: 

- 当y=1时, 损失函数L($\hat{y}$, y) = -ylog($\hat{y}$), 当L尽可能小时, $\hat{y}$就要尽可能大, 因为`sigmoid`函数的取值范围时[0,1], 因此$\hat{y}$无限接近于1, 与y很接近
- 当y=0时, 损失函数L($\hat{y}$, y) = -log(1 - $\hat{y}$), 当L尽可能小时, $\hat{y}$就要尽可能小, 因为`sigmoid`函数的取值范围时[0,1], 因此$\hat{y}$无限接近于0, 与y很接近
- 这门课中有很多函数: **如果y等于1, 就要让$\hat{y}$尽可能大; 如果y等于0, 就要让$\hat{y}$尽可能小**

#### 代价函数

对m个样本的损失函数求和然后除以m, 我们要找到合适的w和b让代价函数J的总代价降到最低

$J ( w , b ) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } L \left( \hat { y } ^ { ( i ) } , y ^ { ( i ) } \right) = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left( - y ^ { ( i ) } \log \hat { y } ^ { ( i ) } - \left( 1 - y ^ { ( i ) } \right) \log \left( 1 - \hat { y } ^ { ( i ) } \right) \right)$

### 梯度下降法

通过最小化代价函数J(w, b)来训练参数w和b

形象化J为![image.png](https://upload-images.jianshu.io/upload_images/12014150-1ed1e8318b91c2ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

代价函数J(w,b)是在水平轴w和b上的曲面, 曲面的高度就是J(w,b)在某点的函数值

- 首先采用随机初始化方法在曲面上随机取一点, 初始化参数w和b(因为函数为凸函数, 因此无论在哪里初始化最终结果大致相同)
- 朝嘴都的下坡方向走一步, 不断迭代
- 知道走到全局最优解或者接近全局最优解的地方

**暂且忽略参数b, 只考虑一维的w**

$w :=\omega-\alpha \frac{d J(a)}{d w}$

- $\alpha$表示学习率, 用来控制补偿
- $\frac{d J(w)}{d w}$是函数J(w)对w求导

- 当随机点位于最小值右侧时, 斜率(导数)大于零, 每次w减小一点, 往左移一点, 直到到达最低点

  ![image.png](https://upload-images.jianshu.io/upload_images/12014150-a8228033527f0571.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

- 当随机点位于最小值左侧时, 斜率(导数)小于零, 每次w增大一点, 往右移一点, 直到到达最低点

  ![image.png](https://upload-images.jianshu.io/upload_images/12014150-7afb2c968168690c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**考虑w和b两个参数**

$w :=w-a \frac{\partial J(w, b)}{\partial w}$

$b :=b-a \frac{\partial J(w, b)}{\partial b}$