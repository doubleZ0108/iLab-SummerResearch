# 对手写数字识别的简单优化

事实上，在没有优化的情况下，两个神经网络的精度已经很高了。MnistInKeras是建立的普通网络，其测试集精度在97.9%左右；而Mnist_cnn利用了卷积网络，其精度已经高达99%，优化空间着实有限。两个网络都不复杂，图片本身也不算大（28*28），所以我认为两个网络训练出的结果应该相差无几，故我把优化的重点放在了前者。

普通网络：![普通网络.PNG](https://upload-images.jianshu.io/upload_images/12014150-4a23b17681ae8965.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

卷积网络：![卷积.PNG](https://upload-images.jianshu.io/upload_images/12014150-0473d013e5daf2d6.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 学习率的优化

我首先想到了优化学习率。查阅资料得知Keras框架下，默认的学习率是0.01，显然，在训练的后期，这个学习率是有可能偏大而使得无法收束于Loss函数的最底端，故采取动态改变学习率的方法来进行优化。主要思想就是在学习过程中，当loss函数值不再下降时，便将学习率变为0.1或0.5倍，使得函数更好的收束。

该方法的实现就是通过调用keras.callbacks中的ReduceLROnPlateau()函数来实现的。重点在于确定该函数众多的参数数值。

* monitor：被监测的量
* factor：每次减少学习率的因子，学习率将以`lr = lr*factor`的形式被减少
* patience：当patience个epoch过去而模型性能不提升时，学习率减少的动作会被触发
* mode：‘auto’，‘min’，‘max’之一，在`min`模式下，如果检测值触发学习率减少。在`max`模式下，当检测值不再上升则触发学习率减少
* epsilon：阈值，用来确定是否进入检测值的“平原区”
* cooldown：学习率减少后，会经过cooldown个epoch才重新进行正常操作
* min_lr：学习率的下限

经过多此调试，并参考原来网络的Loss变化规律。在普通网络中，如下参数设置能得到比较好的效果：

```python
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, epsilon=0.0001, patience=3, mode='min')
```

然后在模型的fit函数中加入该callback即可。此外，为了让该callback被充分的调用，将普通网络中的epoch由原来的4扩大至20，以使得学习率能充分的变小。最后得出结果在98.7%左右，还是有比较明显的提高。![RL 普通网络.PNG](https://upload-images.jianshu.io/upload_images/12014150-e5d0109391e4bc7d.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

而在卷积网络中，我发现其val_loss是在较稳定地减少，扩大Epoch到30发现在20之后便不再下降，即使是减少学习率也如此，说明此时已经到达了最低端，故减少学习率的方法在该网络中起不到多大的作用。



